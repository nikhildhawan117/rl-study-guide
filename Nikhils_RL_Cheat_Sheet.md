# Nikhil's RL Cheat Sheet

> A condensed reference distilled from studying RL for LLM post-training.
> Covers the core ideas behind SFT vs RL, policy gradients, REINFORCE, PPO, GRPO, async RL, and the bias-variance tradeoffs that connect them all.

---

## 1. Bias vs Variance (General ML Refresher)

This is a fundamental tradeoff in machine learning that appears repeatedly in RL algorithm design.

| Concept      | Definition | Example |
|--------------|-----------|---------|
| **Bias**     | Systematic error from wrong modeling assumptions. The model consistently misses the true relationship. | Fitting a linear model to quadratic data. |
| **Variance** | Sensitivity to fluctuations in the training data. A high-variance model changes dramatically depending on *which* data it sees. | A deep decision tree that memorizes training noise and fails on new data. |

**Key relationship:** To reduce bias you typically need a more flexible model, which increases variance. To reduce variance you constrain the model, which increases bias. This tradeoff shows up throughout RL in how we estimate advantages, whether we use learned value functions, how we clip ratios, etc.

> **Common misconception:** Variance is not "noise in the dataset." Noise exists in the data regardless of your model. *Variance* is about how much your *model's predictions* swing when trained on different samples from the same distribution. A high-variance model overfits to that noise; a low-variance model ignores it (but may underfit).

---

## 2. SFT vs RL: The Core Idea

### SFT (Supervised Fine-Tuning)

In SFT, you have a known-correct trajectory $y^* = (y_1^*, y_2^*, \ldots, y_T^*)$. The model learns to reproduce it by maximizing the probability of each correct token given all preceding tokens:

$$\mathcal{L}_{\text{SFT}} = -\sum_{t=1}^{T} \log \pi_\theta(y_t^* \mid x,\, y_{<t}^*)$$

This is a sum of log-probs (equivalent to the log of the product of conditional probabilities, which avoids the numerical underflow you'd get from multiplying many small numbers directly).

### RL (Reinforcement Learning)

In RL, there is **no known-correct trajectory**. Instead:

1. The model *generates* a trajectory $y \sim \pi_\theta(\cdot \mid x)$.
2. A reward signal scores how good the trajectory was.
3. We adjust token probabilities based on that score.

The policy gradient loss looks strikingly similar to SFT:

$$\mathcal{L}_{\text{RL}} = -\mathbb{E}_{y \sim \pi_\theta}\left[\sum_{t=1}^{T} \log \pi_\theta(y_t \mid x,\, y_{<t}) \cdot A(x, y)\right]$$

| | SFT | RL |
|---|---|---|
| **Tokens** | Ground-truth $y^*$ (given) | Sampled $y$ (model-generated) |
| **Weighting** | All tokens weighted equally (implicit weight = 1) | Tokens weighted by advantage $A$ |
| **Signal** | Per-token supervision | Per-trajectory (or per-turn) reward |
| **Gradient direction** | Always push toward the correct token | Push toward tokens that led to good reward; push away from tokens that led to bad reward |

**The punchline:** The gradient computation $\nabla_\theta \log \pi_\theta(y_t \mid s_t)$ is identical in both cases. RL just multiplies it by the advantage.

---

## 3. Why Advantage Instead of Raw Reward?

If we scale log-probs by raw reward $R$:

```
Response A: R = 8.5  →  reinforce all its tokens (by 8.5)
Response B: R = 8.2  →  reinforce all its tokens (by 8.2)
Response C: R = 8.0  →  reinforce all its tokens (by 8.0)
```

Every response gets reinforced. The relative signal is tiny and the gradient magnitudes are dominated by the absolute reward scale.

**With advantage** $A = R - \text{baseline}$:

```
baseline = 8.23

Response A: A = +0.27   →  reinforce (better than average)
Response B: A = -0.03   →  suppress  (worse than average)
Response C: A = -0.23   →  suppress  (worse than average)
```

Now the gradient directly reflects *relative quality*: "Was this response better or worse than expected?"

---

## 4. REINFORCE: The Simplest Policy Gradient

The vanilla REINFORCE algorithm (Williams, 1992) is the most basic form of policy gradient.

$$\nabla_\theta J(\theta) = \mathbb{E}_{y \sim \pi_\theta}\left[\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(y_t \mid s_t) \cdot R(y)\right]$$

**Key properties:**
- **Uses raw return $R$, not advantage.** The original algorithm scales log-probs by the trajectory return directly.
- **Strictly on-policy:** Each batch of trajectories is generated by the current policy, used for one gradient step, then thrown away.
- **No importance sampling, no clipping, no old policy.**

**Why raw reward is a problem:** If rewards are always positive (say, ranging from 5.0 to 10.0), *every* trajectory gets reinforced — even bad ones. Good trajectories are reinforced more, but bad trajectories are never explicitly suppressed. This leads to slow learning and wasted gradient signal.

**Subsequent improvements** (REINFORCE with baseline, REINFORCE++, RLOO) address this by subtracting a baseline to create an advantage, dramatically reducing variance while remaining unbiased. For example:

- **REINFORCE++:** Subtracts a global batch mean and divides by batch std (z-score normalization across the entire batch).
- **RLOO (Leave-One-Out):** For $K$ rollouts per prompt, uses the mean reward of the other $K-1$ rollouts as baseline, giving an unbiased advantage:

$$\hat{A}_i = R_i - \frac{1}{K-1}\sum_{j \neq i} R_j$$

---

## 5. Synchronous vs Asynchronous RL

### Key Terminology

| Term | Meaning |
|------|---------|
| **Prompt** | A single input to generate rollouts from |
| **Episode / Trajectory** | One complete generation for a prompt (start → EOS). *One* trajectory = *one* episode. |
| **Batch** | A collection of trajectories used for one gradient update |
| **Group** | Multiple trajectories for the *same* prompt (GRPO generates G of these) |

> **Note:** An "episode" is a single trajectory, not multiple trajectories per prompt. Multiple rollouts per prompt is typically called a "group" (GRPO) or "sample set."

### Synchronous RL

A clean loop:

```
repeat:
    1. Sample prompts from the dataset
    2. Generate trajectories using current policy π_θ (fill the batch)
    3. Compute advantages for each trajectory
    4. Compute loss = -Σ (log-probs × advantage) across the batch
    5. Gradient update → new π_θ
    6. Discard the batch, repeat with new policy
```

Everything is on-policy: the policy that generated the data is the same policy being updated.

### Asynchronous RL

Two decoupled components:

**Actors** (many):
- Each copies the latest policy weights
- Generates trajectories and sends them (with their log-probs) to the learner
- Keeps generating without waiting for gradient updates

**Learner** (one or few):
- Maintains a buffer of incoming trajectories
- As soon as enough trajectories fill the buffer, computes a gradient step with the current policy
- Does not wait for all actors to finish

**The core problem:** By the time a slow actor finishes a long rollout, the learner may have updated the policy several times. Those trajectories were generated by a *stale* (old) policy — they are **off-policy** samples.

**The fix — importance sampling:** The actor sends back the log-probs from generation time. The learner does a forward pass through the *current* policy on the same trajectory to get updated log-probs, then computes a correction ratio:

$$\rho_t = \frac{\pi_{\theta_\text{new}}(y_t \mid s_t)}{\pi_{\theta_\text{old}}(y_t \mid s_t)} = \exp\!\Big(\log \pi_{\theta_\text{new}}(y_t \mid s_t) - \log \pi_{\theta_\text{old}}(y_t \mid s_t)\Big)$$

### What Does the IS Ratio Mean?

| Ratio | Interpretation |
|------|----------------|
| $\rho > 1$ | The current policy finds this token *more* likely than the old policy did. This token was *underrepresented* relative to the current policy's distribution, so we scale up its contribution. |
| $\rho = 1$ | Same probability under old and new policy. No correction needed. |
| $\rho < 1$ | The current policy finds this token *less* likely. It was *overrepresented* relative to the current distribution, so we scale down its contribution. |

> **Important subtlety:** The IS ratio is a *mathematical correction* for distribution mismatch — it re-weights samples so that the gradient estimate is valid for the *current* policy even though the data came from an *old* policy. It says nothing about whether the token was "good" or "bad"; that's the advantage's job. The ratio only corrects *how much weight* that sample should carry.

---

## 6. PPO: Proximal Policy Optimization

PPO is named for the constraint that the updated policy must remain **proximal** (close) to the policy that generated the rollouts.

### Why PPO Exists

Unlike REINFORCE, PPO **reuses the same batch for multiple gradient steps** (e.g., 4–8 epochs over the same data). This improves sample efficiency but means that after the first step, the current policy $\pi_\theta$ has diverged from the policy $\pi_{\text{old}}$ that generated the data. PPO uses importance sampling plus clipping to keep this under control.

### PPO's Four Main Components

#### (1) Importance Sampling

The probability ratio corrects for off-policyness introduced by reusing data:

$$r_t(\theta) = \frac{\pi_\theta(y_t \mid s_t)}{\pi_{\text{old}}(y_t \mid s_t)}$$

**Problem:** IS corrects bias from off-policy data, but introduces **high variance** — especially when the ratio becomes very large or very small.

#### (2) Clipping

Clip the ratio to bound how much the policy can change in one update:

$$\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}\left[\min\Big(r_t \cdot A_t,\;\; \text{clip}(r_t,\; 1{-}\epsilon,\; 1{+}\epsilon) \cdot A_t\Big)\right]$$

where typically $\epsilon = 0.2$, bounding $r_t$ to $[0.8, 1.2]$.

**Effect:** Reduces the variance that IS introduces (no wild gradient swings from extreme ratios). Adds some bias because we're no longer using the exact IS-corrected objective in the clipped region.

#### (3) KL Divergence Penalty (Optional)

$$\mathcal{L}_{\text{total}} = \mathcal{L}^{\text{CLIP}} - \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$$

where $\pi_{\text{ref}}$ is typically the frozen SFT/base model.

> **Clarification on purpose:** The KL penalty is primarily **regularization**, not variance reduction. Its main jobs are:
> 1. **Prevent reward hacking** — stop the policy from finding adversarial outputs that score high on the reward model but are low quality.
> 2. **Preserve general capabilities** — keep the model close to its pretrained distribution so it doesn't "forget" how to be a good language model.
> 3. **Act as a Bayesian prior** — mathematically equivalent to variational inference, pulling the policy toward the reference.
>
> It does constrain the policy, which indirectly stabilizes training, but this is a side effect of regularization rather than a targeted variance reduction technique.

#### (4) Learned Value Function (Critic)

PPO trains a separate value network $V_\phi(s)$ that estimates the expected future reward from any state:

$$A_t = R_t - V_\phi(s_t) \quad \text{(simple form)}$$

In practice, PPO uses **Generalized Advantage Estimation (GAE)** which smoothly interpolates between low-variance/high-bias (TD) and high-variance/low-bias (Monte Carlo) estimates:

$$\hat{A}_t^{\text{GAE}} = \sum_{l=0}^{T-t} (\gamma\lambda)^l \,\delta_{t+l}, \quad \text{where } \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

**Bias-variance tradeoff of the critic:**
- **Reduces variance** compared to using a sample-based baseline (like GRPO's group mean), because $V(s)$ is trained over many examples and provides a smooth, state-dependent baseline.
- **Introduces bias** because $V_\phi(s)$ is an imperfect approximation of the true expected return. If the critic is poorly trained, the advantage estimates are systematically wrong.
- **Extra cost:** Requires training and maintaining a separate network (significant memory and compute overhead).

### PPO Clipping: What Actually Happens to the Gradients

Let $r_t = \pi_\theta(y_t \mid s_t) / \pi_{\text{old}}(y_t \mid s_t)$ and $A_t$ be the advantage.

The PPO objective is:

$$\mathcal{L} = \min\Big(r_t \cdot A_t,\;\; \text{clip}(r_t,\; 1{-}\epsilon,\; 1{+}\epsilon) \cdot A_t\Big)$$

There are **six cases** to consider. In each, ask: does the `min` select the clipped or unclipped term?

#### When Advantage > 0 (good action, we want to reinforce)

| Range of $r_t$ | Unclipped | Clipped | `min` selects | Gradient? |
|---|---|---|---|---|
| $r_t > 1+\epsilon$ | $r_t A$ — large | $(1+\epsilon) A$ — constant | **Clipped** (smaller) | **Zero** — already overweights this token, no need to reinforce further |
| $1-\epsilon \le r_t \le 1+\epsilon$ | $r_t A$ | $r_t A$ (same) | Either (equal) | **Normal** |
| $r_t < 1-\epsilon$ | $r_t A$ — small | $(1-\epsilon) A$ | **Unclipped** (smaller) | **Normal** — underweights a good token, gradient flows to reinforce it |

#### When Advantage < 0 (bad action, we want to suppress)

| Range of $r_t$ | Unclipped | Clipped | `min` selects | Gradient? |
|---|---|---|---|---|
| $r_t > 1+\epsilon$ | $r_t A$ — very negative | $(1+\epsilon) A$ — less negative | **Unclipped** (more negative = smaller) | **Normal** — overweights a bad token, gradient flows to suppress it |
| $1-\epsilon \le r_t \le 1+\epsilon$ | $r_t A$ | $r_t A$ (same) | Either | **Normal** |
| $r_t < 1-\epsilon$ | $r_t A$ — less negative | $(1-\epsilon) A$ — more negative | **Clipped** (more negative = smaller) | **Zero** — already underweights this bad token, no need to suppress further |

#### The Intuition in One Sentence

Clipping creates a **"no-op zone"**: if the policy has *already* moved in the right direction (reinforcing good tokens or suppressing bad ones) *past the clip boundary*, the gradient shuts off. But if the policy has moved in the *wrong* direction or hasn't moved enough, gradient flows normally. This is a one-sided constraint — it only prevents *excessive* change in the beneficial direction, never blocking corrective change.

---

## 7. GRPO: Group Relative Policy Optimization

GRPO (used in DeepSeek-R1) is a critic-free alternative to PPO.

### Core Idea

Instead of training a value function $V(s)$ to estimate expected reward, GRPO:

1. Generates **$G$ rollouts** (a "group") per prompt
2. Uses the **group mean reward** as the baseline
3. Normalizes advantages as z-scores within the group:

$$\hat{A}_i = \frac{R_i - \mu_G}{\sigma_G}$$

where $\mu_G$ and $\sigma_G$ are the mean and std of the group's rewards.

### GRPO Objective

$$\mathcal{L}^{\text{GRPO}}(\theta) = \mathbb{E}\left[\sum_{i=1}^{G} \min\!\Big(r_t^{(i)} \hat{A}_i,\;\; \text{clip}(r_t^{(i)},\; 1{-}\epsilon,\; 1{+}\epsilon)\,\hat{A}_i\Big)\right] - \beta\, D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$$

GRPO keeps IS, clipping, and KL penalization from PPO. The only major change is how advantages are computed.

### Bias-Variance Tradeoff: GRPO vs PPO

| | PPO (Learned Critic) | GRPO (Group Mean) |
|---|---|---|
| **Bias** | Higher — the critic $V_\phi(s)$ is an imperfect learned approximation of true expected return. Systematic errors propagate into advantage estimates. | Lower — the group mean is a direct Monte Carlo estimate from the policy itself. No function approximation error. |
| **Variance** | Lower — the critic is trained over many batches and provides a smooth, state-conditioned baseline. | Higher — the group mean is computed from only $G$ samples (e.g., 16). Small sample sizes make it a noisy estimate. |
| **Compute** | Expensive — requires training and maintaining a separate value network. | Cheaper — no critic network needed, saving significant memory and GPU. |

> **Nuance:** The tradeoff depends on context. For short-horizon tasks (typical single-turn LLM generations), GRPO often works as well or better because the critic doesn't have enough signal to learn a good value function anyway. For long-horizon tasks with sparse rewards, a well-trained critic can significantly outperform the group-based baseline.

> **On "outliers":** The group mean's variance issue isn't really about outliers per se — it's about **sample size**. With $G = 16$ rollouts, the group mean is a noisy estimate of the true expected reward. A learned value function trained over thousands of batches will be a smoother estimator (at the cost of possible systematic bias from function approximation).

---

## 8. Quick Reference: Connecting the Concepts

```
REINFORCE (1992)
   │  Raw reward scaling → high variance, but simple and on-policy
   │
   ├─► REINFORCE + baseline → variance reduction via advantage
   │     │
   │     ├─► REINFORCE++ → global batch normalization
   │     └─► RLOO → leave-one-out baseline (unbiased)
   │
   ├─► PPO
   │     • Reuses data for multiple epochs (sample efficient)
   │     • IS ratio corrects for off-policy data
   │     • Clipping bounds the ratio → reduces IS variance, adds bias
   │     • Learned critic V(s) → smooth baseline, but adds bias
   │     • KL penalty → regularization (prevents reward hacking)
   │
   └─► GRPO
         • Same clipping + IS + KL as PPO
         • Replaces learned critic with group mean (critic-free)
         • Lower bias, higher variance in advantage estimation
         • Much cheaper (no value network)
```

### Algorithm Comparison Table

| Algorithm | Critic? | IS Ratio? | Clipping? | KL Penalty | Advantage Source | On/Off Policy |
|-----------|---------|-----------|-----------|------------|-----------------|---------------|
| **REINFORCE** | No | No | No | Optional | Raw return | On |
| **REINFORCE++** | No | No | No | In reward | Batch z-score | On |
| **RLOO** | No | No | No | In reward | Leave-one-out mean | On |
| **PPO** | Yes | Yes | Yes | Optional | Learned $V(s)$ + GAE | On (with data reuse) |
| **GRPO** | No | Yes | Yes | Yes | Group mean z-score | On (with data reuse) |

---

## 9. Corrections and Common Pitfalls

A few things that are easy to get wrong:

1. **REINFORCE and baselines:** Vanilla REINFORCE uses raw return, not advantage. Adding a baseline (to get advantage) is an *extension* of REINFORCE, not part of the original algorithm. Importantly, subtracting any state-dependent baseline that doesn't depend on the action preserves the unbiasedness of the gradient estimate (this can be proven by showing the expected gradient of the baseline term is zero).

2. **IS is correction, not "importance weighting" in the intuitive sense.** The IS ratio doesn't tell you whether a token is good or bad — that's the advantage's job. The ratio only corrects the *statistical weight* of each sample to account for the fact that it was drawn from a different distribution than the one we're computing gradients for.

3. **PPO is on-policy, not off-policy.** Even though PPO reuses data for multiple epochs, it's typically classified as on-policy because the data is generated by a recent version of the same policy (not a fundamentally different behavior policy). The off-policyness is small and controlled by clipping.

4. **KL penalty ≠ variance reduction.** The KL penalty against a reference model is regularization. Its primary purpose is preventing reward hacking and preserving pretrained capabilities, not reducing gradient variance. Clipping and the value function baseline are the main variance reduction tools.

5. **"Episode" = one trajectory.** In standard RL terminology, one episode is one complete trajectory (prompt → EOS). Multiple rollouts for the same prompt is a "group" (GRPO) or "sample set," not "many episodes."

---

*Based on the [RL Post-Training Study Guide](./RL_Post_Training_Study_Guide.md) and verified against primary sources.*

*Last updated: February 2026*
