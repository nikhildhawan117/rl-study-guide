# Nikhil's RL Cheat Sheet

> A condensed reference distilled from studying RL for LLM post-training.
> Covers the core ideas behind SFT vs RL, policy gradients, REINFORCE, PPO, GRPO, async RL, and the bias-variance tradeoffs that connect them all.

---

## 1. Bias vs Variance (General ML Refresher)

This is a fundamental tradeoff in machine learning that appears repeatedly in RL algorithm design.

| Concept      | Definition | Example |
|--------------|-----------|---------|
| **Bias**     | Systematic error from wrong modeling assumptions. The model consistently misses the true relationship. | Fitting a linear model to quadratic data. |
| **Variance** | Sensitivity to fluctuations in the training data. A high-variance model changes dramatically depending on *which* data it sees. | A deep decision tree that memorizes training noise and fails on new data. |

**Key relationship:** To reduce bias you typically need a more flexible model, which increases variance. To reduce variance you constrain the model, which increases bias. This tradeoff shows up throughout RL in how we estimate advantages, whether we use learned value functions, how we clip ratios, etc.

> **Common misconception:** Variance is not "noise in the dataset." Noise exists in the data regardless of your model. *Variance* is about how much your *model's predictions* swing when trained on different samples from the same distribution. A high-variance model overfits to that noise; a low-variance model ignores it (but may underfit).

---

## 2. SFT vs RL: The Core Idea

### SFT (Supervised Fine-Tuning)

In SFT, you have a known-correct trajectory $y^* = (y_1^*, y_2^*, \ldots, y_T^*)$. The model learns to reproduce it by maximizing the probability of each correct token given all preceding tokens:

$$\mathcal{L}_{\text{SFT}} = -\sum_{t=1}^{T} \log \pi_\theta(y_t^* \mid x,\, y_{<t}^*)$$

This is a sum of log-probs (equivalent to the log of the product of conditional probabilities, which avoids the numerical underflow you'd get from multiplying many small numbers directly).

### RL (Reinforcement Learning)

In RL, there is **no known-correct trajectory**. Instead:

1. The model *generates* a trajectory $y \sim \pi_\theta(\cdot \mid x)$.
2. A reward signal scores how good the trajectory was.
3. We adjust token probabilities based on that score.

The policy gradient loss looks strikingly similar to SFT:

$$\mathcal{L}_{\text{RL}} = -\mathbb{E}_{y \sim \pi_\theta}\left[\sum_{t=1}^{T} \log \pi_\theta(y_t \mid x,\, y_{<t}) \cdot A(x, y)\right]$$

| | SFT | RL |
|---|---|---|
| **Tokens** | Ground-truth $y^*$ (given) | Sampled $y$ (model-generated) |
| **Weighting** | All tokens weighted equally (implicit weight = 1) | Tokens weighted by advantage $A$ |
| **Signal** | Per-token supervision | Per-trajectory (or per-turn) reward |
| **Gradient direction** | Always push toward the correct token | Push toward tokens that led to good reward; push away from tokens that led to bad reward |

**The punchline:** The gradient computation $\nabla_\theta \log \pi_\theta(y_t \mid s_t)$ is identical in both cases. RL just multiplies it by the advantage.

---

## 3. Why Advantage Instead of Raw Reward?

If we scale log-probs by raw reward $R$:

```
Response A: R = 8.5  →  reinforce all its tokens (by 8.5)
Response B: R = 8.2  →  reinforce all its tokens (by 8.2)
Response C: R = 8.0  →  reinforce all its tokens (by 8.0)
```

Every response gets reinforced. The relative signal is tiny and the gradient magnitudes are dominated by the absolute reward scale.

**With advantage** $A = R - \text{baseline}$:

```
baseline = 8.23

Response A: A = +0.27   →  reinforce (better than average)
Response B: A = -0.03   →  suppress  (worse than average)
Response C: A = -0.23   →  suppress  (worse than average)
```

Now the gradient directly reflects *relative quality*: "Was this response better or worse than expected?"

---

## 4. REINFORCE: The Simplest Policy Gradient

The vanilla REINFORCE algorithm (Williams, 1992) is the most basic form of policy gradient.

$$\nabla_\theta J(\theta) = \mathbb{E}_{y \sim \pi_\theta}\left[\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(y_t \mid s_t) \cdot R(y)\right]$$

**Key properties:**
- **Uses raw return $R$, not advantage.** The original algorithm scales log-probs by the trajectory return directly.
- **Strictly on-policy:** Each batch of trajectories is generated by the current policy, used for one gradient step, then thrown away.
- **No importance sampling, no clipping, no old policy.**

**Why raw reward is a problem:** If rewards are always positive (say, ranging from 5.0 to 10.0), *every* trajectory gets reinforced — even bad ones. Good trajectories are reinforced more, but bad trajectories are never explicitly suppressed. This leads to slow learning and wasted gradient signal.

**Subsequent improvements** (REINFORCE with baseline, REINFORCE++, RLOO) address this by subtracting a baseline to create an advantage, dramatically reducing variance while remaining unbiased. For example:

- **REINFORCE++:** Subtracts a global batch mean and divides by batch std (z-score normalization across the entire batch).
- **RLOO (Leave-One-Out):** For $K$ rollouts per prompt, uses the mean reward of the other $K-1$ rollouts as baseline, giving an unbiased advantage:

$$\hat{A}_i = R_i - \frac{1}{K-1}\sum_{j \neq i} R_j$$

---

## 5. Synchronous vs Asynchronous RL

### Key Terminology

| Term | Meaning |
|------|---------|
| **Prompt** | A single input to generate rollouts from |
| **Episode / Trajectory** | One complete generation for a prompt (start → EOS). *One* trajectory = *one* episode. |
| **Batch** | A collection of trajectories used for one gradient update |
| **Group** | Multiple trajectories for the *same* prompt (GRPO generates G of these) |

> **Note:** An "episode" is a single trajectory, not multiple trajectories per prompt. Multiple rollouts per prompt is typically called a "group" (GRPO) or "sample set."

### Synchronous RL

A clean loop:

```
repeat:
    1. Sample prompts from the dataset
    2. Generate trajectories using current policy π_θ (fill the batch)
    3. Compute advantages for each trajectory
    4. Compute loss = -Σ (log-probs × advantage) across the batch
    5. Gradient update → new π_θ
    6. Discard the batch, repeat with new policy
```

Everything is on-policy: the policy that generated the data is the same policy being updated.

### Asynchronous RL

Two decoupled components:

**Actors** (many):
- Each copies the latest policy weights
- Generates trajectories and sends them (with their log-probs) to the learner
- Keeps generating without waiting for gradient updates

**Learner** (one or few):
- Maintains a buffer of incoming trajectories
- As soon as enough trajectories fill the buffer, computes a gradient step with the current policy
- Does not wait for all actors to finish

**The core problem:** By the time a slow actor finishes a long rollout, the learner may have updated the policy several times. Those trajectories were generated by a *stale* (old) policy — they are **off-policy** samples.

**The fix — importance sampling:** The actor sends back the log-probs from generation time. The learner does a forward pass through the *current* policy on the same trajectory to get updated log-probs, then computes a correction ratio:

$$\rho_t = \frac{\pi_{\theta_\text{new}}(y_t \mid s_t)}{\pi_{\theta_\text{old}}(y_t \mid s_t)} = \exp\!\Big(\log \pi_{\theta_\text{new}}(y_t \mid s_t) - \log \pi_{\theta_\text{old}}(y_t \mid s_t)\Big)$$

### What Does the IS Ratio Mean?

| Ratio | Interpretation |
|------|----------------|
| $\rho > 1$ | The current policy finds this token *more* likely than the old policy did. This token was *underrepresented* relative to the current policy's distribution, so we scale up its contribution. |
| $\rho = 1$ | Same probability under old and new policy. No correction needed. |
| $\rho < 1$ | The current policy finds this token *less* likely. It was *overrepresented* relative to the current distribution, so we scale down its contribution. |

> **Important subtlety:** The IS ratio is a *mathematical correction* for distribution mismatch — it re-weights samples so that the gradient estimate is valid for the *current* policy even though the data came from an *old* policy. It says nothing about whether the token was "good" or "bad"; that's the advantage's job. The ratio only corrects *how much weight* that sample should carry.

---

## 6. PPO: Proximal Policy Optimization

PPO is named for the constraint that the updated policy must remain **proximal** (close) to the policy that generated the rollouts.

### Why PPO Exists

Unlike REINFORCE, PPO **reuses the same batch for multiple gradient steps** (e.g., 4–8 epochs over the same data). This improves sample efficiency but means that after the first step, the current policy $\pi_\theta$ has diverged from the policy $\pi_{\text{old}}$ that generated the data. PPO uses importance sampling plus clipping to keep this under control.

### PPO's Four Main Components

#### (1) Importance Sampling

The probability ratio corrects for off-policyness introduced by reusing data:

$$r_t(\theta) = \frac{\pi_\theta(y_t \mid s_t)}{\pi_{\text{old}}(y_t \mid s_t)}$$

**Problem:** IS corrects bias from off-policy data, but introduces **high variance** — especially when the ratio becomes very large or very small.

#### (2) Clipping

Clip the ratio to bound how much the policy can change in one update:

$$\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}\left[\min\Big(r_t \cdot A_t,\;\; \text{clip}(r_t,\; 1{-}\epsilon,\; 1{+}\epsilon) \cdot A_t\Big)\right]$$

where typically $\epsilon = 0.2$, bounding $r_t$ to $[0.8, 1.2]$.

**Effect:** Reduces the variance that IS introduces (no wild gradient swings from extreme ratios). Adds some bias because we're no longer using the exact IS-corrected objective in the clipped region.

#### (3) KL Divergence Penalty (Optional)

$$\mathcal{L}_{\text{total}} = \mathcal{L}^{\text{CLIP}} - \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$$

where $\pi_{\text{ref}}$ is typically the frozen SFT/base model.

> **Clarification on purpose:** The KL penalty is primarily **regularization**, not variance reduction. Its main jobs are:
> 1. **Prevent reward hacking** — stop the policy from finding adversarial outputs that score high on the reward model but are low quality.
> 2. **Preserve general capabilities** — keep the model close to its pretrained distribution so it doesn't "forget" how to be a good language model.
> 3. **Act as a Bayesian prior** — mathematically equivalent to variational inference, pulling the policy toward the reference.
>
> It does constrain the policy, which indirectly stabilizes training, but this is a side effect of regularization rather than a targeted variance reduction technique.

#### (4) Learned Value Function (Critic)

PPO trains a separate value network $V_\phi(s)$ that estimates the expected future reward from any state:

$$A_t = R_t - V_\phi(s_t) \quad \text{(simple form)}$$

In practice, PPO uses **Generalized Advantage Estimation (GAE)** which smoothly interpolates between low-variance/high-bias (TD) and high-variance/low-bias (Monte Carlo) estimates:

$$\hat{A}_t^{\text{GAE}} = \sum_{l=0}^{T-t} (\gamma\lambda)^l \,\delta_{t+l}, \quad \text{where } \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

**Bias-variance tradeoff of the critic:**
- **Reduces variance** compared to using a sample-based baseline (like GRPO's group mean), because $V(s)$ is trained over many examples and provides a smooth, state-dependent baseline.
- **Introduces bias** because $V_\phi(s)$ is an imperfect approximation of the true expected return. If the critic is poorly trained, the advantage estimates are systematically wrong.
- **Extra cost:** Requires training and maintaining a separate network (significant memory and compute overhead).

### PPO Clipping: What Actually Happens to the Gradients

Let $r_t = \pi_\theta(y_t \mid s_t) / \pi_{\text{old}}(y_t \mid s_t)$ and $A_t$ be the advantage.

The PPO objective is:

$$\mathcal{L} = \min\Big(r_t \cdot A_t,\;\; \text{clip}(r_t,\; 1{-}\epsilon,\; 1{+}\epsilon) \cdot A_t\Big)$$

There are **six cases** to consider. In each, ask: does the `min` select the clipped or unclipped term?

#### When Advantage > 0 (good action, we want to reinforce)

| Range of $r_t$ | Unclipped | Clipped | `min` selects | Gradient? |
|---|---|---|---|---|
| $r_t > 1+\epsilon$ | $r_t A$ — large | $(1+\epsilon) A$ — constant | **Clipped** (smaller) | **Zero** — already overweights this token, no need to reinforce further |
| $1-\epsilon \le r_t \le 1+\epsilon$ | $r_t A$ | $r_t A$ (same) | Either (equal) | **Normal** |
| $r_t < 1-\epsilon$ | $r_t A$ — small | $(1-\epsilon) A$ | **Unclipped** (smaller) | **Normal** — underweights a good token, gradient flows to reinforce it |

#### When Advantage < 0 (bad action, we want to suppress)

| Range of $r_t$ | Unclipped | Clipped | `min` selects | Gradient? |
|---|---|---|---|---|
| $r_t > 1+\epsilon$ | $r_t A$ — very negative | $(1+\epsilon) A$ — less negative | **Unclipped** (more negative = smaller) | **Normal** — overweights a bad token, gradient flows to suppress it |
| $1-\epsilon \le r_t \le 1+\epsilon$ | $r_t A$ | $r_t A$ (same) | Either | **Normal** |
| $r_t < 1-\epsilon$ | $r_t A$ — less negative | $(1-\epsilon) A$ — more negative | **Clipped** (more negative = smaller) | **Zero** — already underweights this bad token, no need to suppress further |

#### The Intuition in One Sentence

Clipping creates a **"no-op zone"**: if the policy has *already* moved in the right direction (reinforcing good tokens or suppressing bad ones) *past the clip boundary*, the gradient shuts off. But if the policy has moved in the *wrong* direction or hasn't moved enough, gradient flows normally. This is a one-sided constraint — it only prevents *excessive* change in the beneficial direction, never blocking corrective change.

### PPO Value Function Clipping

PPO doesn't just clip the policy ratio — it also clips the **value function** update. This is a detail that's easy to miss but matters for stability.

#### The Problem

The critic $V_\phi(s)$ is trained to predict expected returns, and its predictions feed directly into the advantage estimate $A_t = R_t - V_\phi(s_t)$. If the critic makes a large jump between updates (e.g., suddenly predicting very different values), the advantage estimates swing wildly too, destabilizing the policy gradient. Since PPO reuses the same batch for multiple epochs, the critic can overfit to that batch and drift far from its old predictions.

#### The Clipped Value Loss

The fix mirrors the policy clipping idea. Define a clipped value prediction:

$$V_{\text{clip}} = V_{\text{old}}(s_t) + \text{clip}\big(V_{\text{new}}(s_t) - V_{\text{old}}(s_t),\; -\epsilon,\; +\epsilon\big)$$

Then the value loss takes the **max** of the clipped and unclipped squared errors:

$$\mathcal{L}^{V}(\phi) = \frac{1}{2}\, \max\!\Big(\big(R_t - V_{\text{new}}(s_t)\big)^2,\;\; \big(R_t - V_{\text{clip}}(s_t)\big)^2\Big)$$

| Term | What it does |
|------|-------------|
| $V_{\text{old}}(s_t)$ | The critic's prediction from the start of this PPO epoch (frozen) |
| $V_{\text{new}}(s_t)$ | The critic's current prediction (being updated) |
| $V_{\text{clip}}(s_t)$ | The current prediction, but clamped to within $\pm\epsilon$ of the old one |
| $R_t$ | The actual return (target) |

#### Why `max` Instead of `min`?

The policy objective uses `min` (pessimistic — pick the *worse* objective to prevent overconfident updates). The value loss uses `max` (also pessimistic — pick the *larger* loss to prevent the critic from "cheating" by jumping to a very different value). If the clipped prediction happens to be closer to the target, `max` ensures we still use the unclipped loss, preventing the critic from taking a shortcut through a large value jump.

#### Why This Matters

Without value clipping, the critic can change dramatically across PPO epochs on the same batch. Since advantages are computed as $A_t = R_t - V(s_t)$, a wildly shifting critic means wildly shifting advantages, which means unstable policy updates — even though the policy ratio itself is clipped. Value clipping ensures that **both halves** of the actor-critic system are constrained to change slowly.

> **Note:** Some implementations omit value clipping and still get good results. The original Schulman et al. PPO paper mentions it but doesn't emphasize it. Empirical studies (e.g., "Implementation Matters in Deep RL") found it can help in some environments but isn't universally necessary. It's one of those implementation details that can matter a lot in practice.

---

## 7. GRPO: Group Relative Policy Optimization

GRPO (used in DeepSeek-R1) is a critic-free alternative to PPO.

### Core Idea

Instead of training a value function $V(s)$ to estimate expected reward, GRPO:

1. Generates **$G$ rollouts** (a "group") per prompt
2. Uses the **group mean reward** as the baseline
3. Normalizes advantages as z-scores within the group:

$$\hat{A}_i = \frac{R_i - \mu_G}{\sigma_G}$$

where $\mu_G$ and $\sigma_G$ are the mean and std of the group's rewards.

### GRPO Objective

$$\mathcal{L}^{\text{GRPO}}(\theta) = \mathbb{E}\left[\sum_{i=1}^{G} \min\!\Big(r_t^{(i)} \hat{A}_i,\;\; \text{clip}(r_t^{(i)},\; 1{-}\epsilon,\; 1{+}\epsilon)\,\hat{A}_i\Big)\right] - \beta\, D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$$

GRPO keeps IS, clipping, and KL penalization from PPO. The only major change is how advantages are computed.

### Bias-Variance Tradeoff: REINFORCE vs PPO vs GRPO

| | REINFORCE (Raw Return) | PPO (Learned Critic) | GRPO (Group Mean) |
|---|---|---|---|
| **Bias** | Lowest — uses the actual return $R$ directly. No function approximation, no sampling-based baseline. The gradient estimate is unbiased. | Higher — the critic $V_\phi(s)$ is an imperfect learned approximation of true expected return. Systematic errors propagate into advantage estimates. | Lower than PPO — the group mean is a direct Monte Carlo estimate from the policy itself. No function approximation error. |
| **Variance** | Highest — raw return includes all the noise from the entire trajectory. No baseline to center the signal, so gradients swing wildly between updates. | Lowest — the critic is trained over many batches and provides a smooth, state-conditioned baseline. Clipping and data reuse further stabilize updates. | Middle — the group mean reduces variance vs raw return (it's a baseline), but is computed from only $G$ samples (e.g., 16), so it's noisier than a learned critic. |
| **Compute** | Cheapest — no extra networks, no data reuse, one gradient step per batch. | Most expensive — requires training and maintaining a separate value network, plus multiple epochs per batch. | Middle — no critic network (saves memory/GPU), but generates $G$ rollouts per prompt and does multiple epochs per batch. |

> **Nuance:** The tradeoff depends on context. For short-horizon tasks (typical single-turn LLM generations), GRPO often works as well or better because the critic doesn't have enough signal to learn a good value function anyway. For long-horizon tasks with sparse rewards, a well-trained critic can significantly outperform the group-based baseline. REINFORCE variants (REINFORCE++, RLOO) sit in between — they add baselines to reduce variance substantially while staying cheaper than PPO.

> **On "outliers":** The group mean's variance issue isn't really about outliers per se — it's about **sample size**. With $G = 16$ rollouts, the group mean is a noisy estimate of the true expected reward. A learned value function trained over thousands of batches will be a smoother estimator (at the cost of possible systematic bias from function approximation). REINFORCE with no baseline is even noisier — it doesn't subtract anything at all.

---

## 8. DPO: Direct Preference Optimization

DPO (Rafailov et al., 2023) is an "RL-free" approach to preference tuning. It achieves the same goal as RLHF (aligning a model with human preferences) but **skips the reward model and the RL loop entirely**.

### The Problem DPO Solves

The standard RLHF pipeline has three stages:

```
1. Train a reward model on human preference data
2. Use RL (PPO) to optimize the policy against that reward model
3. Add KL penalty to prevent divergence from reference
```

This is expensive, unstable, and requires careful hyperparameter tuning. DPO collapses all three stages into a single supervised learning objective.

### The Key Insight

Start from the KL-regularized RL objective that RLHF tries to solve:

$$\max_\pi \; \mathbb{E}_{x,\, y \sim \pi}\big[r(x, y)\big] - \beta \, D_{\text{KL}}\big(\pi \| \pi_{\text{ref}}\big)$$

This has a **closed-form optimal solution**:

$$\pi^*(y \mid x) = \frac{1}{Z(x)} \; \pi_{\text{ref}}(y \mid x) \; \exp\!\Big(\frac{r(x, y)}{\beta}\Big)$$

where $Z(x)$ is a normalizing constant (partition function).

The magic trick: **rearrange this to solve for the reward** instead of the policy:

$$r(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)$$

This says: if you know the optimal policy, you implicitly know the reward. The language model *is* secretly a reward model.

### The DPO Loss

Plug this implicit reward into the Bradley-Terry preference model $P(y_w \succ y_l) = \sigma(r(x, y_w) - r(x, y_l))$:

$$\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x,\, y_w,\, y_l)}\left[\log \sigma\!\Big(\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\Big)\right]$$

The $Z(x)$ terms cancel (they're the same for both responses to the same prompt), leaving a clean loss.

**In plain English:** For each preference pair $(y_w, y_l)$:

1. Compute how much more the current model likes $y_w$ vs the reference: $\beta \log \frac{\pi_\theta(y_w)}{\pi_{\text{ref}}(y_w)}$
2. Compute the same for $y_l$: $\beta \log \frac{\pi_\theta(y_l)}{\pi_{\text{ref}}(y_l)}$
3. The difference should be positive (model should prefer the winner more than the reference does)
4. Pass through sigmoid + log — this is just binary cross-entropy

### What DPO Is Really Doing

Define the **implicit reward** of the model:

$$\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$$

This is how much more (or less) the model likes a response compared to the reference, scaled by $\beta$. The DPO loss is simply:

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}\left[\log \sigma\big(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)\big)\right]$$

This is binary classification: "does the model's implicit reward correctly rank the preferred response above the rejected one?"

### Advantages of DPO

- **No sampling during training** — purely supervised on static preference pairs
- **No reward model** — eliminates a whole model and training stage
- **No value function** — no critic network needed
- **Simpler and more stable** — standard classification loss, no RL instabilities
- **KL regularization is built-in** — the $\pi_{\text{ref}}$ terms act as an implicit KL constraint

### Limitations of DPO

- **Offline only** — trains on a fixed dataset of preference pairs. Cannot explore or generate new responses during training, unlike PPO/GRPO which sample on-policy.
- **Requires global coverage** — the training data must adequately cover the test distribution. If the preference data is narrow, DPO can't generalize beyond it (online RL only needs partial coverage).
- **Prone to overfitting** — can overfit to the preference dataset, especially with deterministic or noisy preferences. May lead to mode collapse.
- **No iterative self-improvement** — unlike RL methods, DPO can't discover better strategies through exploration. It can only learn to distinguish between responses already in the dataset.

### When to Use DPO

DPO shines when you have **high-quality, diverse preference data** and want a simple, stable training pipeline. It's less suitable when you need the model to explore and improve beyond what's in the training data (e.g., reasoning tasks where GRPO/PPO excel).

---

## 9. IPO: Identity Preference Optimization

IPO (Azar et al., 2023, DeepMind) addresses a theoretical weakness in DPO's foundations.

### The Problem with DPO's Assumptions

DPO relies on the **Bradley-Terry model**, which assumes that pairwise preferences can be explained by pointwise reward scores:

$$P(y_w \succ y_l) = \sigma\big(r(y_w) - r(y_l)\big)$$

This assumes preferences are **transitive** (if A > B and B > C, then A > C) and can always be reduced to scalar scores. But real human preferences are often:
- **Noisy** — the same annotator might flip their preference on a re-evaluation
- **Intransitive** — humans might prefer A over B, B over C, but C over A
- **Context-dependent** — not reducible to fixed scalar scores

DPO inherits these assumptions and can overfit to them, especially on noisy preference data.

### The ΨPO Framework

IPO is a special case of a more general framework called **ΨPO**, which unifies preference optimization methods:

$$\mathcal{L}_{\Psi\text{PO}}(\theta) = \mathbb{E}_{(x,\, y_w,\, y_l)}\left[\Psi\!\left(\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\right)\right]$$

Different choices of $\Psi$ give different algorithms:

| $\Psi$ function | Algorithm |
|---|---|
| $\Psi(u) = -\log \sigma(\beta u)$ | **DPO** — derived from Bradley-Terry + KL-regularized RL |
| $\Psi(u) = (u - 1/2\tau)^2$ | **IPO** — directly optimizes pairwise preferences |

### What the IPO Loss Does

The IPO loss:

$$\mathcal{L}_{\text{IPO}}(\theta) = \mathbb{E}\left[\left(\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} - \frac{1}{2\tau}\right)^2\right]$$

**In plain English:** IPO wants the log-probability gap between the preferred and rejected responses (relative to the reference) to be exactly $\frac{1}{2\tau}$. Not infinity, not as large as possible — a specific finite target.

This is critically different from DPO, which (via the sigmoid) pushes the gap toward infinity. That's why DPO can overfit: it keeps trying to make the gap bigger and bigger, eventually overfitting to the training preferences.

### IPO vs DPO: Key Differences

| | DPO | IPO |
|---|---|---|
| **Preference model** | Bradley-Terry (requires pointwise rewards) | Direct pairwise preferences (no pointwise reduction) |
| **Loss shape** | Log-sigmoid — pushes reward gap toward $\infty$ | Squared error — pushes reward gap toward a finite target $\frac{1}{2\tau}$ |
| **Over-training** | Prone to overfitting as gap is pushed ever larger | Robust — naturally stops when gap reaches target |
| **Regularization** | Implicit (through $\pi_{\text{ref}}$ ratio) | Built into the squared loss (finite target) |
| **Noisy preferences** | Sensitive — tries to perfectly separate all pairs | Robust — the finite target tolerates noise |

### When to Use IPO

IPO is preferred when:
- **Preferences are noisy** — human annotators disagree or are inconsistent
- **Over-training is a concern** — you want a method that naturally plateaus
- **Preferences don't fit Bradley-Terry** — intransitive or context-dependent preferences

DPO is preferred when:
- **Preferences are clean and consistent** — the Bradley-Terry assumption holds
- **You want maximum separation** — pushing the model as far as possible toward preferred responses

---

## 10. Quick Reference: Connecting the Concepts

```
Policy Gradient Methods (online, generate + score + update)
═══════════════════════════════════════════════════════════
REINFORCE (1992)
   │  Raw reward scaling → high variance, but simple and on-policy
   │
   ├─► REINFORCE + baseline → variance reduction via advantage
   │     │
   │     ├─► REINFORCE++ → global batch normalization
   │     └─► RLOO → leave-one-out baseline (unbiased)
   │
   ├─► PPO
   │     • Reuses data for multiple epochs (sample efficient)
   │     • IS ratio corrects for off-policy data
   │     • Clipping bounds the ratio → reduces IS variance, adds bias
   │     • Learned critic V(s) + value clipping → smooth baseline
   │     • KL penalty → regularization (prevents reward hacking)
   │
   └─► GRPO
         • Same clipping + IS + KL as PPO
         • Replaces learned critic with group mean (critic-free)
         • Lower bias, higher variance in advantage estimation
         • Much cheaper (no value network)

Preference Optimization Methods (offline, no RL loop)
═════════════════════════════════════════════════════════
DPO
   │  Implicit reward from policy ratios
   │  Binary cross-entropy on preference pairs
   │  Simple + stable, but offline only
   │
   └─► IPO
         • Fixes DPO's over-training problem
         • Squared loss with finite target (not sigmoid → ∞)
         • More robust to noisy/intransitive preferences
```

### Algorithm Comparison Table

| Algorithm | Critic? | IS Ratio? | Clipping? | KL Penalty | Advantage Source | On/Off Policy |
|-----------|---------|-----------|-----------|------------|-----------------|---------------|
| **REINFORCE** | No | No | No | Optional | Raw return | On |
| **REINFORCE++** | No | No | No | In reward | Batch z-score | On |
| **RLOO** | No | No | No | In reward | Leave-one-out mean | On |
| **PPO** | Yes | Yes | Yes | Optional | Learned $V(s)$ + GAE | On (with data reuse) |
| **GRPO** | No | Yes | Yes | Yes | Group mean z-score | On (with data reuse) |
| **DPO** | No | No | No | Implicit ($\pi_{\text{ref}}$) | N/A — no RL | Offline |
| **IPO** | No | No | No | In loss | N/A — no RL | Offline |

---

## 11. Corrections and Common Pitfalls

A few things that are easy to get wrong:

1. **REINFORCE and baselines:** Vanilla REINFORCE uses raw return, not advantage. Adding a baseline (to get advantage) is an *extension* of REINFORCE, not part of the original algorithm. Importantly, subtracting any state-dependent baseline that doesn't depend on the action preserves the unbiasedness of the gradient estimate (this can be proven by showing the expected gradient of the baseline term is zero).

2. **IS is correction, not "importance weighting" in the intuitive sense.** The IS ratio doesn't tell you whether a token is good or bad — that's the advantage's job. The ratio only corrects the *statistical weight* of each sample to account for the fact that it was drawn from a different distribution than the one we're computing gradients for.

3. **PPO is on-policy, not off-policy.** Even though PPO reuses data for multiple epochs, it's typically classified as on-policy because the data is generated by a recent version of the same policy (not a fundamentally different behavior policy). The off-policyness is small and controlled by clipping.

4. **KL penalty ≠ variance reduction.** The KL penalty against a reference model is regularization. Its primary purpose is preventing reward hacking and preserving pretrained capabilities, not reducing gradient variance. Clipping and the value function baseline are the main variance reduction tools.

5. **"Episode" = one trajectory.** In standard RL terminology, one episode is one complete trajectory (prompt → EOS). Multiple rollouts for the same prompt is a "group" (GRPO) or "sample set," not "many episodes."

---

*Based on the [RL Post-Training Study Guide](./RL_Post_Training_Study_Guide.md) and verified against primary sources.*

*Last updated: February 2026*
